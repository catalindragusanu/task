# Ollama Configuration
# Make sure Ollama is installed and running: https://ollama.ai
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Popular model options:
# - llama2 (default, good balance)
# - mistral (fast and capable)
# - codellama (for code-related tasks)
# - llama2:13b (larger, more capable)

# Flask Configuration
FLASK_ENV=development
FLASK_DEBUG=True
